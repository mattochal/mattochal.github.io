<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Mateusz Ochal">

    <link rel="icon" href="../../img/favicon.png">
    <title>Mateusz Ochal</title>

    <!-- Bootstrap Core CSS -->
    <link href="../../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../../css/grayscale.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css2?family=Blinker:wght@100&family=Ubuntu&display=swap" rel="stylesheet" type="text/css">

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>
    
    <script type="text/javascript"
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>

</head>

<!-- BODY -->
<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

  <!-- Main Section -->
  <section id="about" class="container content-section text-center my-section my-body">
    <div class="row">
      <div class="col-lg-10 col-lg-offset-1 blog">
        <h2 style="text-align: center;">Highlights from ICLR 2021 conference</h2>
        <h3 style="text-align: center;">Papers</h3>

        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=D3PcGLdMx0" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3256" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">MELR: Meta-Learning via Modeling Episode-Level Relationships for Few-Shot Learning <br><i>Nanyi Fei, Zhiwu Lu, Tao Xiang, Songfang Huang</i></p>
          </div>
          <div class="small-blog-left">
            <p>This work proposes Modeling Episode-Level Relationship (MELR) which changes the way that tasks are sampled during meta-training and uses Cross-Episode Attention Module (CEAM) and Consistency Regularization (CECR) to improve performance. <i>Task-Sampling.</i> trains models by deliberately sampling two tasks per learning iteration, instead of one like in standard episodic meta-training. The query and support sets for the two tasks share the same classes but contain different instances. <i>Cross-Episode Attention Module (CEAM) </i>is an attention module over the support sets. <i>Cross-Episode Consistency Regularization (CECR) </i>encorages the model to produce similar prototypes across the two episodes.</p>
            <img src="melr.jpg" class="blog">
          </div>
        </div>

        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=9z_dNsC4B5t" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3313" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">MetaNorm: Learning to Normalize Few-Shot Batches Across Domains<br><i>Yingjun Du, Xiantong Zhen, Ling Shao, Cees G. M. Snoek</i></p>
          </div>
          <div class="small-blog-left">
            <p>This work proposes an alternative to BatchNorm, TaskNorm, Transductive Batch Norm and others. Meta-Norm is a non-trasductive method which learns the Batch Norm statistics via meta-learning.<p>
            <img src="metanorm.jpg" class="blog">
          </div>
        </div>

        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=Ti87Pv5Oc8" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/2848" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Meta-Learning with Neural Tangent Kernels<br><i>Yufan Zhou, Zhenyi Wang, Jiayi Xian, Changyou Chen, Jinhui Xu</i></p>
          </div>
          <div class="small-blog-left">
           <p> This work proposes two new methods offering SOTA performance, including on OoD tasks, and robustness against adveserial attacks. Their Meta-RKHS-I algorithm uses two terms: 1) encouraging the models to perform well without adaptation, and 2) encouraging the model to perform large fine-tuning steps to perform fast adaptation. The algorithm is closely related to MAML. Meta-RKHS-II is a closed-form adapation algorithm. The two algorithms are related to each other and their difference between objectives can be bounded.</p>
           <img src="rkhsI.jpg" class="blog" alt="Meta-RKHS-I">
           <img src="rkhsII.jpg" class="blog" alt="Meta-RKHS-II">
           <img src="rkhs_tbl.jpg" class="blog">
          </div>
        </div>

        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=-QxT4mJdijq" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/2607" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Meta-Learning Symmetries by Reparameterization<br><i>Allan Zhou, Tom Knowles, Chelsea Finn</i></p>
          </div>
          <div class="small-blog-left">
            <p>This work proposes Meta-Learning Symmetries by Reparameterization (MSR) which meta-learns equivarient structure to exploit symmartries in the task distribution. (Note: equivarience = varience to the same degree, meaning transformation in the input result in symetric transformations in the output). Conv layers are translation equivarient but are not equivarient natively with rotations, reflections, and scalings. As a result, networks need to be trained through data augmentations, or employ specialized equivariant layers / achitectures. This work proposed MSR, which enforces weight sharing patterns. In the inner loop they only update $v$, keeping $U$ fixed. In the outer-loop they learn equivariance $U$ that learns symetries in the task distributions. MSR is able to learn 2D rotation equivarience, and perform better in augmented FSL tasks (where the query set is the augmented version of the support set).</p>
            <img src="msr.jpg" class="blog">
          </div>
        </div>

        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=vujTf_I8Kmc" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3322" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Attentional Constellation Nets for Few-Shot Learning<br><i>Weijian Xu, Yifan Xu, Huaijin Wang, Zhuowen Tu</i></p>
          </div>
          <div class="small-blog-left"><p>
            Traditional Constellation Model (TCM) extracts key point of interests from an image and uses k-Means clustering to construct a bag-of-visual words. This is then encoded into a shape and modeled by a Gaussian Distribution. This work integrates constellation model into the convolutional neutal networks, which enables the model to discover new visual-words and their relationship. Their constilation modules are inserted after each conv layer, and have two parts: Cell Feature Clustering (CFC, mimics the clustering step in TCM) and Cell Rotation Modelling (CRM, which replaces the shape modelling part in TCM). CRM uses a lot of attention. 
            <img src="acn_fsl.jpg" class="blog">
          </p>
          </div>
        </div>


        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=3jjmdp7Hha" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/2541" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Meta Back-Translation<br><i>Hieu Pham, Xinyi Wang, Yiming Yang, Graham Neubig</i></p>
          </div>
          <div class="small-blog-left"><p>
            Use meta-training to do multi-lingual translation. 
          </p>
            <img src="mbt.jpg" class="blog">
          </div>
        </div>

        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=l-PrrQrK0QR" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3183" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Meta-learning with Negative Learning Rates<br><i>Alberto Bernacchia</i></p>
          </div>
          <div class="small-blog-left"><p>
            This work shows that setting a negative learning rate in the inner loop of MAML leads to bettwe performance on linear and non-linear regression tasks. 
            <img src="neg_lr.jpg" class="blog">
          </p>
          </div>
        </div>

        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=CR1XOQ0UTh-" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/2801" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Contrastive Learning with Hard Negative Samples<br><i>Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka</i></p>
          </div>
          <div class="small-blog-left"><p>
            The idea in contrastive learning is to sample positive and negative pairs of images with respect to some anchor image, and optimize the feature extractor / classifier by driving the feature representations of negative-anchor pairs away from each other, while driving the feature representations of positive-anchor pairs closer together. The pairs are commonly sampled uniformily at random. The papar points out this that this is redundent in situations where the feature extractor has already learned to appropiately drive the representations closer/futher away for positive/negative pairs. This provides little useful gradient signal since the problem is already effectily solved for those samples. In this work, the authors propose a way for the model to focus on the "hard" negative samples which the model is currently wrong on. The hardness is solved by the inner dot product (cosine distance) such that closer the points are to the anchor point the more frequently they appear. The strength of 'hardness' is scaled by the parameter $\beta$ and is also conditions on class labels (approximated by the Positive Unlabelled learning) since some classes are supposed to appear closer to the anchor than others. 
            <img src="contrast.jpg" class="blog">
          </p>
          </div>


          <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=vYeQQ29Tbvx" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3292" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Contrastive Learning with Hard Negative Samples<br><i>Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka</i></p>
          </div>
          <div class="small-blog-left"><p>
            BatchNorm Learns sparse representations.
            <img src="contrast.jpg" class="blog">
          </p>
          </div>
        </div>

          
        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=pW2Q2xLwIMD" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/2535" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Few-Shot Learning via Learning the Representation, Provably<br><i>Simon Shaolei Du, Wei Hu, Sham M. Kakade, Jason D. Lee, Qi Lei</i></p>
          </div>
          <div class="small-blog-left">
          </div>
        </div>


        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=lgNx56yZh8a" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3334" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Bayesian Few-Shot Classification with One-vs-Each PÃ³lya-Gamma Augmented Gaussian Processes<br><i>Jake Snell, Richard Zemel</i></p>
          </div>
          <div class="small-blog-left">
          </div>
        </div>

        
        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=bJxgv5C3sYc" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3163" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Few-Shot Bayesian Optimization with Deep Kernel Surrogates<br><i>Martin Wistuba, Josif Grabocka</i></p>
          </div>
          <div class="small-blog-left"><p>
            
          </p>
          </div>
        </div>

        <!-- -------------------------------------------------------------------------------- -->
        <div class="topmarg" style="text-align:center;">
          <div>
            <a href="https://openreview.net/forum?id=l-PrrQrK0QR" class="btn btn-default btn-sm">Paper</a>
            <a href="https://iclr.cc/virtual/2021/poster/3251" class="btn btn-default btn-sm">Presentation</a>
          </div>
          <div>
            <p class="small">Dataset Meta-Learning from Kernel Ridge-Regression<br><i>Timothy Nguyen, Zhourong Chen, Jaehoon Lee</i></p>
          </div>
          <div class="small-blog-left">
          </div>
        </div>

        </div>
      </div>

    </div>
  </section>


  <!-- Footer -->
  <footer>
      <div class="container text-center">
          <p>Copyright &copy; Mateusz Ochal 2021</p>
      </div>
  </footer>

  <!-- jQuery -->
  <script src="js/jquery.js"></script>

  <!-- Bootstrap Core JavaScript -->
  <script src="js/bootstrap.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="js/jquery.easing.min.js"></script>

  <!-- Google Maps API Key - Use your own API key to enable the map feature. More information on the Google Maps API can be found at https://developers.google.com/maps/ -->
  <script type="text/javascript" src="https://maps.googleapis.com/maps/api/js?key=AIzaSyCRngKslUGJTlibkQ3FkfTxj3Xss1UlZDA&sensor=false"></script>

  <!-- Custom Theme JavaScript -->
  <script src="js/grayscale.js"></script>

</body>
</html>
